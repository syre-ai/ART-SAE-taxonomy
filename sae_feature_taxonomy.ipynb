{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUIs_w6UZSoc"
   },
   "source": [
    "# ART-Organized SAE Feature Taxonomy\n",
    "\n",
    "Cluster pre-trained SAE decoder weight vectors from **Pythia** models using Adaptive Resonance Theory (ART) modules to build hierarchical feature taxonomies.\n",
    "\n",
    "**Pipeline:** SAE W_dec → optional PCA → ART clustering → hierarchical taxonomy via SMART\n",
    "\n",
    "**Key idea:** ART's vigilance parameter provides a single, principled knob for controlling cluster granularity — something SAEs lack. SMART builds a hierarchy by stacking multiple vigilance levels.\n",
    "\n",
    "**Supported models:** Pythia-70M (512-dim, 32k features), Pythia-410M (1024-dim, 65k features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvpA7ONAZSod",
    "outputId": "91fa49ee-f56d-4863-bd01-eccb51a1ebc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "import sys, os\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    # Clone or pull the repo, then cd into it\n",
    "    if os.path.exists(\"art_utils.py\"):\n",
    "        # Already inside the repo (cell re-run)\n",
    "        !git pull\n",
    "    elif os.path.exists(\"ART-SAE-taxonomy\"):\n",
    "        os.chdir(\"ART-SAE-taxonomy\")\n",
    "        !git pull\n",
    "    else:\n",
    "        !git clone https://github.com/syre-ai/ART-SAE-taxonomy.git\n",
    "        os.chdir(\"ART-SAE-taxonomy\")\n",
    "    !pip install -q artlib==0.1.7 eai-sparsify transformers torch umap-learn plotly tqdm pandas\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Force-reload local modules so git pull changes take effect\n",
    "import importlib\n",
    "import gpu_fuzzy_art, art_utils\n",
    "importlib.reload(gpu_fuzzy_art)\n",
    "importlib.reload(art_utils)\n",
    "\n",
    "from art_utils import (\n",
    "    create_art_module,\n",
    "    create_smart_model,\n",
    "    extract_hierarchy_labels,\n",
    "    list_available_modules,\n",
    ")\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6-WJVFqVZSoe"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION — edit this cell to change experiments\n",
    "# ============================================================\n",
    "\n",
    "# --- Model ---\n",
    "LM_MODEL = \"EleutherAI/pythia-410m\"     # LM for logit lens interpretation\n",
    "SAE_MODEL = \"EleutherAI/sae-pythia-410m-65k\"  # SAE decoder weights to cluster\n",
    "LAYER_IDX = 12             # Middle layer (410M has 24 layers, 70M has 6)\n",
    "\n",
    "# --- Clustering ---\n",
    "MODULE_NAME = \"GPUFuzzyART\"  # GPU-accelerated FuzzyART (see list_available_modules())\n",
    "PARAM_OVERRIDES = {\"rho\": 0.15}       # e.g. {\"rho\": 0.8}\n",
    "PCA_DIMS = None            # Set to int (e.g. 50, 100, 200) to reduce dims; None = use raw dims\n",
    "QUICK_TEST = True          # Subsample to 1000 features for fast iteration\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint restore disabled (USE_CHECKPOINT=False) — run full pipeline\n"
     ]
    }
   ],
   "source": [
    "# --- Checkpoint Restore (skip Cells 4-6 if checkpoint exists) ---\n",
    "# Set USE_CHECKPOINT = True to load saved pipeline state instead of recomputing.\n",
    "# This saves ~15 minutes by skipping SAE loading, logit lens, and junk filtering.\n",
    "\n",
    "import os, torch\n",
    "\n",
    "USE_CHECKPOINT = False  # Set True to restore from checkpoint\n",
    "CHECKPOINT_PATH = \"checkpoints/pipeline_state.pt\"\n",
    "\n",
    "if USE_CHECKPOINT and os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"Loading checkpoint from {CHECKPOINT_PATH}...\")\n",
    "    ckpt = torch.load(CHECKPOINT_PATH, weights_only=False)\n",
    "\n",
    "    # Verify config matches\n",
    "    ckpt_cfg = ckpt[\"config\"]\n",
    "    assert ckpt_cfg[\"lm_model\"] == LM_MODEL, \\\n",
    "        f\"Checkpoint LM mismatch: {ckpt_cfg['lm_model']} vs {LM_MODEL}\"\n",
    "    assert ckpt_cfg[\"sae_model\"] == SAE_MODEL, \\\n",
    "        f\"Checkpoint SAE mismatch: {ckpt_cfg['sae_model']} vs {SAE_MODEL}\"\n",
    "\n",
    "    # Restore tensors\n",
    "    W_dec_full = ckpt[\"W_dec_full\"]\n",
    "    top5_ids = ckpt[\"top5_ids\"]\n",
    "    unembed_weights = ckpt[\"unembed_weights\"]\n",
    "    final_ln_weight = ckpt[\"final_ln_weight\"]\n",
    "    final_ln_bias = ckpt[\"final_ln_bias\"]\n",
    "    valid_indices = ckpt[\"valid_indices\"]\n",
    "\n",
    "    # Rebuild final_ln as a simple module for logit lens\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LM_MODEL)\n",
    "    # Create a minimal layer norm with saved weights\n",
    "    import torch.nn as nn\n",
    "    final_ln = nn.LayerNorm(final_ln_weight.shape[0])\n",
    "    final_ln.weight.data = final_ln_weight\n",
    "    final_ln.bias.data = final_ln_bias\n",
    "    final_ln.eval()\n",
    "\n",
    "    # Rebuild filtered/subsampled data\n",
    "    import numpy as np\n",
    "    W_dec_raw = W_dec_full.numpy()\n",
    "    kept_indices = valid_indices.numpy()\n",
    "    W_dec_filtered = W_dec_raw[kept_indices]\n",
    "    rng = np.random.default_rng(RANDOM_SEED)\n",
    "    if QUICK_TEST:\n",
    "        n_sample = min(1000, W_dec_filtered.shape[0])\n",
    "        sub_idx = rng.choice(W_dec_filtered.shape[0], size=n_sample, replace=False)\n",
    "        idx_original = kept_indices[sub_idx]\n",
    "        W_dec = W_dec_filtered[sub_idx]\n",
    "    else:\n",
    "        perm = rng.permutation(W_dec_filtered.shape[0])\n",
    "        idx_original = kept_indices[perm]\n",
    "        W_dec = W_dec_filtered[perm]\n",
    "    idx = idx_original\n",
    "\n",
    "    # Run preprocessing (PCA) inline\n",
    "    if PCA_DIMS is not None and PCA_DIMS < W_dec.shape[1]:\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=PCA_DIMS, random_state=RANDOM_SEED)\n",
    "        X_reduced = pca.fit_transform(W_dec)\n",
    "    else:\n",
    "        X_reduced = W_dec.copy()\n",
    "    data_dim = X_reduced.shape[1]\n",
    "\n",
    "    print(f\"Checkpoint restored: {W_dec_full.shape[0]} features, \"\n",
    "          f\"{len(kept_indices)} valid, {X_reduced.shape[0]} selected, \"\n",
    "          f\"{data_dim}-dim\")\n",
    "    print(f\"Config: {ckpt_cfg}\")\n",
    "    print(\">>> Skip Cells 4-7 and proceed to Cell 8 (Create ART module)\")\n",
    "else:\n",
    "    if USE_CHECKPOINT:\n",
    "        print(f\"Checkpoint not found at {CHECKPOINT_PATH} — run full pipeline (Cells 4-7)\")\n",
    "    else:\n",
    "        print(\"Checkpoint restore disabled (USE_CHECKPOINT=False) — run full pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567,
     "referenced_widgets": [
      "7a769163cb994ee696540defcc9d8004",
      "5a242e5ceeb740c38e63698011acf3ef",
      "d72ee8a8142043ebbeee65093359f5f2",
      "761ec4428f4b4988b8fd52bc70ffb031",
      "d7d7c472c38a4b48be349c63ce78f770",
      "c90b6d555b414422a581c46868e21f3b",
      "b37e81525f1a4e5d9c72df74623abc33",
      "93ee07cbc2f9448fa4f6e76b76f78ae8",
      "7749dfcf282c4a4892d11e87cda41aac",
      "a8f1273328ec49cb9fcbbe4d1148d499",
      "9fb1b49baed04eccb905a69f14afc6e4",
      "5d445b318e01416d8581bbd483e7c834",
      "779842a0a3e84318882b30f4a46822fc",
      "1c61fba917ca465a8c80644c091fce75",
      "8b7c0b01735447538ac73a6c791abeac",
      "37456a9f41144975bb00ad364ebb58f2",
      "8c08137aac294db58eea77f93bc09ac2",
      "2f1111c1450745e9a44fe732351927c2",
      "3bcccf6d8db8493c8f5d500e54650a94",
      "517bbe701d874447a6f2156a65625cb3",
      "b31a19cf294541c7823bd65584dc0aa8",
      "4a8a6f8155b44f81b525c136c7735407"
     ]
    },
    "id": "hkGUSk8QZSof",
    "outputId": "51dad020-bd12-4e6f-d265-fbbae5f0f2aa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ee2bf48c2346ff91e2b3ee072b6a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 49 files:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded W_dec from EleutherAI/sae-pythia-410m-65k\n",
      "  Layer: layers.12.mlp, shape: (65536, 1024) (features × hidden_dim)\n",
      "  Available keys: ['layers.0.mlp', 'layers.1.mlp', 'layers.10.mlp', 'layers.11.mlp', 'layers.12.mlp', 'layers.13.mlp', 'layers.14.mlp', 'layers.15.mlp', 'layers.16.mlp', 'layers.17.mlp', 'layers.18.mlp', 'layers.19.mlp', 'layers.2.mlp', 'layers.20.mlp', 'layers.21.mlp', 'layers.22.mlp', 'layers.23.mlp', 'layers.3.mlp', 'layers.4.mlp', 'layers.5.mlp', 'layers.6.mlp', 'layers.7.mlp', 'layers.8.mlp', 'layers.9.mlp']\n"
     ]
    }
   ],
   "source": [
    "# --- Load SAE decoder weights ---\n",
    "from sparsify import Sae\n",
    "\n",
    "saes = Sae.load_many(SAE_MODEL)\n",
    "\n",
    "# Detect key format: \"layers.N\" (70M) vs \"layers.N.mlp\" (410M)\n",
    "layer_key = f\"layers.{LAYER_IDX}\"\n",
    "if layer_key not in saes:\n",
    "    layer_key = f\"layers.{LAYER_IDX}.mlp\"\n",
    "W_dec_raw = saes[layer_key].W_dec.detach().numpy()\n",
    "print(f\"Loaded W_dec from {SAE_MODEL}\")\n",
    "print(f\"  Layer: {layer_key}, shape: {W_dec_raw.shape} (features × hidden_dim)\")\n",
    "print(f\"  Available keys: {sorted(saes.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aced3a7017a147a494c24c369dd2b4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26f73029d904aa3b90f2388f6306ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa20e632e8f41fb844a041f6977dbc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1884aae7e35d487689b1dd4d3954490f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fc63050a07401b9edfa91b252afb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/911M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM: EleutherAI/pythia-410m, vocab=50304, hidden=1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit lens: computed top-5 tokens for 65536 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Junk filter: 65536 total → 65534 kept, 2 filtered (0.0% junk)\n",
      "  0/5 junk tokens: 63560 features\n",
      "  1/5 junk tokens: 1935 features\n",
      "  2/5 junk tokens: 39 features\n",
      "  3/5 junk tokens: 2 features\n",
      "  4/5 junk tokens: 0 features\n",
      "  5/5 junk tokens: 0 features\n",
      "\n",
      "Quick test mode: subsampled to 1000 features (from 65534 non-junk)\n"
     ]
    }
   ],
   "source": [
    "# --- Filter junk features + subsample ---\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load LM (reused by Cell 11 for interpretation)\n",
    "tokenizer = AutoTokenizer.from_pretrained(LM_MODEL)\n",
    "lm = AutoModelForCausalLM.from_pretrained(LM_MODEL)\n",
    "lm.eval()\n",
    "lm.float()\n",
    "\n",
    "# Pythia models have UNTIED embeddings — use embed_out + final_layer_norm\n",
    "unembed_weights = lm.embed_out.weight.detach()  # (vocab_size, hidden_dim)\n",
    "final_ln = lm.gpt_neox.final_layer_norm\n",
    "print(f\"LM: {LM_MODEL}, vocab={unembed_weights.shape[0]}, hidden={unembed_weights.shape[1]}\")\n",
    "\n",
    "# --- Batch logit lens to classify all features (chunked to avoid OOM) ---\n",
    "W_dec_full = torch.from_numpy(W_dec_raw).float()\n",
    "n_features = W_dec_full.shape[0]\n",
    "top5_ids = torch.zeros(n_features, 5, dtype=torch.long)\n",
    "\n",
    "LOGIT_BATCH = 4096\n",
    "with torch.no_grad():\n",
    "    for start in range(0, n_features, LOGIT_BATCH):\n",
    "        end = min(start + LOGIT_BATCH, n_features)\n",
    "        normed = final_ln(W_dec_full[start:end])\n",
    "        logits = normed @ unembed_weights.T\n",
    "        top5_ids[start:end] = logits.topk(5, dim=1).indices\n",
    "        del normed, logits  # free memory between chunks\n",
    "print(f\"Logit lens: computed top-5 tokens for {n_features} features\")\n",
    "\n",
    "# --- Junk token classifier (minimal: only truly uninformative tokens) ---\n",
    "def is_junk_token(token_id):\n",
    "    decoded = tokenizer.decode([token_id])\n",
    "    if '\\ufffd' in decoded:        return True   # byte-fallback (broken encoding)\n",
    "    if '<|' in decoded:            return True   # special tokens (<|endoftext|> etc.)\n",
    "    return False\n",
    "\n",
    "# Classify each feature: junk if >=3 of top-5 tokens are junk\n",
    "junk_counts = torch.zeros(n_features, dtype=torch.long)\n",
    "for i in range(n_features):\n",
    "    junk_counts[i] = sum(is_junk_token(tid.item()) for tid in top5_ids[i])\n",
    "\n",
    "is_junk_feature = junk_counts >= 3  # >=60% junk\n",
    "keep_mask = ~is_junk_feature\n",
    "\n",
    "n_total = W_dec_raw.shape[0]\n",
    "n_kept = keep_mask.sum().item()\n",
    "n_filtered = n_total - n_kept\n",
    "print(f\"Junk filter: {n_total} total → {n_kept} kept, {n_filtered} filtered ({n_filtered/n_total:.1%} junk)\")\n",
    "\n",
    "# Distribution of junk counts\n",
    "for j in range(6):\n",
    "    nj = (junk_counts == j).sum().item()\n",
    "    print(f\"  {j}/5 junk tokens: {nj} features\")\n",
    "\n",
    "# --- Apply filter, then subsample/shuffle ---\n",
    "kept_indices = torch.where(keep_mask)[0].numpy()\n",
    "W_dec_filtered = W_dec_raw[kept_indices]\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "if QUICK_TEST:\n",
    "    n_sample = min(1000, W_dec_filtered.shape[0])\n",
    "    idx = rng.choice(W_dec_filtered.shape[0], size=n_sample, replace=False)\n",
    "    # Map back to original feature indices\n",
    "    idx_original = kept_indices[idx]\n",
    "    W_dec = W_dec_filtered[idx]\n",
    "    print(f\"\\nQuick test mode: subsampled to {W_dec.shape[0]} features (from {W_dec_filtered.shape[0]} non-junk)\")\n",
    "else:\n",
    "    perm = rng.permutation(W_dec_filtered.shape[0])\n",
    "    idx_original = kept_indices[perm]\n",
    "    W_dec = W_dec_filtered[perm]\n",
    "    print(f\"\\nShuffled {W_dec.shape[0]} non-junk features\")\n",
    "\n",
    "# idx tracks the original feature indices (for logit lens lookups in Cell 11)\n",
    "idx = idx_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LIyjTESZSof"
   },
   "outputs": [],
   "source": "# --- Preprocessing: optional PCA ---\n\nnorms = np.linalg.norm(W_dec, axis=1)\nprint(f\"W_dec L2 norms — min: {norms.min():.4f}, max: {norms.max():.4f}, \"\n      f\"mean: {norms.mean():.4f}, std: {norms.std():.6f}\")\n\nif PCA_DIMS is not None and PCA_DIMS < W_dec.shape[1]:\n    # Fit full PCA to report variance landscape, then truncate\n    max_components = min(W_dec.shape[0], W_dec.shape[1])\n    pca_full = PCA(n_components=min(max_components, max(PCA_DIMS, 200)), random_state=RANDOM_SEED)\n    pca_full.fit(W_dec)\n    cumvar = np.cumsum(pca_full.explained_variance_ratio_)\n\n    print(f\"Variance landscape ({len(cumvar)} components computed):\")\n    for thresh in [0.50, 0.80, 0.90, 0.95, 0.99]:\n        n_dims = np.searchsorted(cumvar, thresh) + 1\n        if n_dims <= len(cumvar):\n            print(f\"  {thresh:.0%} variance: {n_dims} dims\")\n        else:\n            print(f\"  {thresh:.0%} variance: >{len(cumvar)} dims (max computed: {cumvar[-1]:.1%})\")\n\n    # Use the requested number of dims\n    X_reduced = pca_full.transform(W_dec)[:, :PCA_DIMS]\n    explained = cumvar[PCA_DIMS - 1]\n    print(f\"\\nPCA: {W_dec.shape[1]} → {PCA_DIMS} dims ({explained:.1%} variance retained)\")\nelse:\n    X_reduced = W_dec.copy()\n    print(f\"No PCA applied. Using {X_reduced.shape[1]} dims.\")\n\ndata_dim = X_reduced.shape[1]\nprint(f\"Final data shape: {X_reduced.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to checkpoints/pipeline_state.pt (455.5 MB)\n",
      "  W_dec_full: torch.Size([65536, 1024])\n",
      "  top5_ids: torch.Size([65536, 5])\n",
      "  valid_indices: 65534 features\n",
      "To restore: set USE_CHECKPOINT=True in Cell 2, then run Cell 3\n"
     ]
    }
   ],
   "source": [
    "# --- Checkpoint Save ---\n",
    "# Saves pipeline state so future runs can skip Cells 4-6 (~15 min → ~3 sec).\n",
    "# Run this after Cells 4-6 complete successfully.\n",
    "\n",
    "import torch, os\n",
    "\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "CHECKPOINT_PATH = \"checkpoints/pipeline_state.pt\"\n",
    "\n",
    "checkpoint = {\n",
    "    \"W_dec_full\": W_dec_full,\n",
    "    \"top5_ids\": top5_ids,\n",
    "    \"unembed_weights\": unembed_weights,\n",
    "    \"final_ln_weight\": final_ln.weight.data.clone(),\n",
    "    \"final_ln_bias\": final_ln.bias.data.clone(),\n",
    "    \"valid_indices\": torch.from_numpy(kept_indices),\n",
    "    \"config\": {\n",
    "        \"lm_model\": LM_MODEL,\n",
    "        \"sae_model\": SAE_MODEL,\n",
    "        \"layer_idx\": LAYER_IDX,\n",
    "        \"n_features\": W_dec_full.shape[0],\n",
    "        \"hidden_dim\": W_dec_full.shape[1],\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, CHECKPOINT_PATH)\n",
    "file_size = os.path.getsize(CHECKPOINT_PATH) / (1024 * 1024)\n",
    "print(f\"Checkpoint saved to {CHECKPOINT_PATH} ({file_size:.1f} MB)\")\n",
    "print(f\"  W_dec_full: {W_dec_full.shape}\")\n",
    "print(f\"  top5_ids: {top5_ids.shape}\")\n",
    "print(f\"  valid_indices: {len(kept_indices)} features\")\n",
    "print(f\"To restore: set USE_CHECKPOINT=True in Cell 2, then run Cell 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "50u0PNLMZSof"
   },
   "outputs": [],
   "source": "# ============================================================\n# PCA DIMENSION SWEEP\n# ============================================================\n# Tests different PCA dimensions to find the best setting for\n# cluster interpretability. Runs SMART hierarchy + differential\n# logit lens for each setting, then prints a comparison.\n#\n# Requires cells 1-5 to have run (W_dec, W_dec_full, idx, etc.)\n# ============================================================\n\nimport json\nimport time\n\n# --- Sweep configuration ---\nPCA_DIMS_SWEEP = [None, 32, 64, 128, 256]\nRHO_VALUES = [0.2, 0.5, 0.8]\nN_DRILL = 3     # Drill into top N L0 clusters to show L1 children\nTOP_K = 10      # Tokens per cluster interpretation\n\n# --- Interpretation function ---\nglobal_mean_dir = W_dec_full.mean(dim=0)\n\ndef _get_tokens(feature_indices, top_k=10):\n    \"\"\"Differential logit lens.\"\"\"\n    dirs = W_dec_full[feature_indices]\n    diff_dir = dirs.mean(dim=0) - global_mean_dir\n    with torch.no_grad():\n        normed = final_ln(diff_dir.unsqueeze(0))\n        logits = (normed @ unembed_weights.T).squeeze(0)\n    topk = logits.topk(top_k)\n    return [tokenizer.decode([tid]) for tid in topk.indices.tolist()]\n\n# --- PCA variance landscape (compute once) ---\nprint(\"Computing PCA variance landscape...\")\nmax_pca = min(W_dec.shape[0], W_dec.shape[1])\npca_landscape = PCA(n_components=min(max_pca, 512), random_state=RANDOM_SEED)\npca_landscape.fit(W_dec)\ncumvar_full = np.cumsum(pca_landscape.explained_variance_ratio_)\n\nprint(f\"Variance landscape ({len(cumvar_full)} components):\")\nfor thresh in [0.50, 0.80, 0.90, 0.95, 0.99]:\n    n_dims = np.searchsorted(cumvar_full, thresh) + 1\n    if n_dims <= len(cumvar_full):\n        print(f\"  {thresh:.0%}: {n_dims} dims\")\n    else:\n        print(f\"  {thresh:.0%}: >{len(cumvar_full)} dims (max: {cumvar_full[-1]:.1%})\")\n\n# --- Run sweep ---\nall_results = {}\n\nfor pca_dims in PCA_DIMS_SWEEP:\n    print(\"\\n\" + \"=\" * 70)\n    if pca_dims is None:\n        label = f\"None (raw {W_dec.shape[1]}d)\"\n        X_sweep = W_dec.copy()\n        var_explained = 1.0\n    else:\n        var_explained = float(cumvar_full[pca_dims - 1])\n        label = f\"{pca_dims}d ({var_explained:.1%} var)\"\n        X_sweep = pca_landscape.transform(W_dec)[:, :pca_dims]\n    print(f\"PCA = {label}\")\n    print(\"=\" * 70)\n\n    dim = X_sweep.shape[1]\n    t0 = time.time()\n\n    # Create and fit SMART\n    smart_sweep = create_smart_model(MODULE_NAME, dim=dim, rho_values=RHO_VALUES)\n    X_prepared = smart_sweep.prepare_data(X_sweep)\n    smart_sweep.fit(X_prepared, verbose=True)\n\n    hierarchy_sweep = extract_hierarchy_labels(smart_sweep)\n    fit_time = time.time() - t0\n\n    # Cluster counts\n    cluster_counts = []\n    for i, rho in enumerate(RHO_VALUES):\n        n = len(np.unique(hierarchy_sweep[:, i]))\n        cluster_counts.append(n)\n    print(f\"  Clusters: L0={cluster_counts[0]}, L1={cluster_counts[1]}, L2={cluster_counts[2]}  ({fit_time:.1f}s)\")\n\n    # L0 taxonomy\n    l0_ids = sorted(np.unique(hierarchy_sweep[:, 0]))\n    l0_results = {}\n\n    print(f\"\\n  L0 CLUSTERS ({len(l0_ids)}):\")\n    for c in l0_ids:\n        mask = hierarchy_sweep[:, 0] == c\n        n_feat = int(mask.sum())\n        feature_indices = idx[mask]\n        n_l1 = len(np.unique(hierarchy_sweep[mask, 1]))\n        tokens = _get_tokens(feature_indices, top_k=TOP_K)\n        l0_results[int(c)] = {\"size\": n_feat, \"n_l1\": n_l1, \"tokens\": tokens}\n        print(f\"    L0:{c} ({n_feat} feat, {n_l1} L1): {tokens}\")\n\n    # Drill into top N L0 clusters by size\n    l0_by_size = sorted(l0_ids, key=lambda c: -(hierarchy_sweep[:, 0] == c).sum())\n\n    for c in l0_by_size[:N_DRILL]:\n        mask = hierarchy_sweep[:, 0] == c\n        n_feat = int(mask.sum())\n        feature_indices = idx[mask]\n        tokens = _get_tokens(feature_indices, top_k=TOP_K)\n\n        print(f\"\\n  DRILL L0:{c} ({n_feat} feat): {tokens}\")\n\n        child_ids, child_counts = np.unique(hierarchy_sweep[mask, 1], return_counts=True)\n        order = np.argsort(-child_counts)\n        max_show = min(10, len(child_ids))\n        print(f\"  Top {max_show}/{len(child_ids)} L1 children:\")\n\n        for ci in order[:max_show]:\n            cid = child_ids[ci]\n            child_mask = mask & (hierarchy_sweep[:, 1] == cid)\n            child_features = idx[child_mask]\n            child_tokens = _get_tokens(child_features, top_k=TOP_K)\n            print(f\"    L1:{cid} ({child_counts[ci]} feat): {child_tokens}\")\n\n    # Store results\n    pca_key = str(pca_dims) if pca_dims is not None else \"None\"\n    all_results[pca_key] = {\n        \"pca_dims\": pca_dims,\n        \"var_explained\": round(float(var_explained), 4),\n        \"cluster_counts\": cluster_counts,\n        \"fit_time\": round(fit_time, 1),\n        \"l0_clusters\": l0_results,\n    }\n\n# --- Summary comparison ---\nprint(\"\\n\\n\" + \"=\" * 70)\nprint(\"SWEEP SUMMARY\")\nprint(\"=\" * 70)\nheader = f\"{'PCA':<10} {'Var%':<8} {'L0':<5} {'L1':<5} {'L2':<6} {'Time':<7} {'Largest L0 cluster tokens'}\"\nprint(header)\nprint(\"-\" * len(header))\nfor pca_key, res in all_results.items():\n    cc = res[\"cluster_counts\"]\n    largest = max(res[\"l0_clusters\"].values(), key=lambda x: x[\"size\"])\n    pct = largest[\"size\"] / sum(v[\"size\"] for v in res[\"l0_clusters\"].values())\n    tok_str = \", \".join(largest[\"tokens\"][:5])\n    print(f\"{pca_key:<10} {res['var_explained']:.1%}   {cc[0]:<5} {cc[1]:<5} {cc[2]:<6} {res['fit_time']:.1f}s   [{pct:.0%}] {tok_str}\")\n\nprint(f\"\\n[%] = fraction of all features in the largest L0 cluster (lower = less catch-all)\")\n\n# Save results\nwith open(\"pca_sweep_results.json\", \"w\") as f:\n    json.dump(all_results, f, indent=2, default=str)\nprint(f\"Results saved to pca_sweep_results.json\")\nprint(\"\\nDone! Review the L0 cluster tokens above to identify which PCA setting\")\nprint(\"produces the most semantically distinct and interpretable clusters.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ksj67P6XZSof"
   },
   "outputs": [],
   "source": "# (Skipped — flat clustering now handled by PCA sweep in Cell 8)\npass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpMHsmeeZSog"
   },
   "outputs": [],
   "source": "# (Skipped — UMAP visualization now handled by PCA sweep in Cell 8)\npass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZ4bYIZMZSog"
   },
   "outputs": [],
   "source": "# (Skipped — cluster size distribution now handled by PCA sweep in Cell 8)\npass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZijdnpOZSog"
   },
   "outputs": [],
   "source": "# (Skipped — SMART hierarchy now handled by PCA sweep in Cell 8)\npass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# (Skipped — interpretation function now defined in PCA sweep Cell 8)\npass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# (Skipped — results JSON now saved by PCA sweep in Cell 8)\npass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# (Skipped — L0 taxonomy now displayed by PCA sweep in Cell 8)\npass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueLAu-g7ZSog"
   },
   "outputs": [],
   "source": "# (Skipped — explore_cluster now handled by PCA sweep drill-down in Cell 8)\npass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhqQHZC4ZSoh"
   },
   "outputs": [],
   "source": "# (Skipped — vigilance sweep subsumed by PCA sweep in Cell 8)\npass"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBlRV1R6ZSoh"
   },
   "source": [
    "## Phase 2: Checkpoint Comparison (Stub)\n",
    "\n",
    "**Goal:** Track how SAE feature categories evolve across Pythia-70M training checkpoints.\n",
    "\n",
    "**Approach:**\n",
    "1. Load Pythia-70M at multiple checkpoints (e.g., steps 1000, 10000, 50000, 143000)\n",
    "2. Extract SAE decoder weights at each checkpoint\n",
    "3. Use ART's `partial_fit()` to incrementally update the taxonomy\n",
    "4. Track: resonant features (stable), new categories (plastic), dormant categories (lost)\n",
    "\n",
    "This leverages ART's core strength — stability-plasticity balance — which SAEs lack entirely (they must retrain from scratch).\n",
    "\n",
    "*Implementation in Phase 2.*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (GPU)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1c61fba917ca465a8c80644c091fce75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3bcccf6d8db8493c8f5d500e54650a94",
      "max": 38,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_517bbe701d874447a6f2156a65625cb3",
      "value": 38
     }
    },
    "2f1111c1450745e9a44fe732351927c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "37456a9f41144975bb00ad364ebb58f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3bcccf6d8db8493c8f5d500e54650a94": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a8a6f8155b44f81b525c136c7735407": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "517bbe701d874447a6f2156a65625cb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5a242e5ceeb740c38e63698011acf3ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c90b6d555b414422a581c46868e21f3b",
      "placeholder": "​",
      "style": "IPY_MODEL_b37e81525f1a4e5d9c72df74623abc33",
      "value": "Download complete: "
     }
    },
    "5d445b318e01416d8581bbd483e7c834": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_779842a0a3e84318882b30f4a46822fc",
       "IPY_MODEL_1c61fba917ca465a8c80644c091fce75",
       "IPY_MODEL_8b7c0b01735447538ac73a6c791abeac"
      ],
      "layout": "IPY_MODEL_37456a9f41144975bb00ad364ebb58f2"
     }
    },
    "761ec4428f4b4988b8fd52bc70ffb031": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8f1273328ec49cb9fcbbe4d1148d499",
      "placeholder": "​",
      "style": "IPY_MODEL_9fb1b49baed04eccb905a69f14afc6e4",
      "value": " 2.42G/? [00:40&lt;00:00, 128MB/s]"
     }
    },
    "7749dfcf282c4a4892d11e87cda41aac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "779842a0a3e84318882b30f4a46822fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c08137aac294db58eea77f93bc09ac2",
      "placeholder": "​",
      "style": "IPY_MODEL_2f1111c1450745e9a44fe732351927c2",
      "value": "Fetching 38 files: 100%"
     }
    },
    "7a769163cb994ee696540defcc9d8004": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5a242e5ceeb740c38e63698011acf3ef",
       "IPY_MODEL_d72ee8a8142043ebbeee65093359f5f2",
       "IPY_MODEL_761ec4428f4b4988b8fd52bc70ffb031"
      ],
      "layout": "IPY_MODEL_d7d7c472c38a4b48be349c63ce78f770"
     }
    },
    "8b7c0b01735447538ac73a6c791abeac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b31a19cf294541c7823bd65584dc0aa8",
      "placeholder": "​",
      "style": "IPY_MODEL_4a8a6f8155b44f81b525c136c7735407",
      "value": " 38/38 [00:25&lt;00:00,  2.05it/s]"
     }
    },
    "8c08137aac294db58eea77f93bc09ac2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93ee07cbc2f9448fa4f6e76b76f78ae8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "9fb1b49baed04eccb905a69f14afc6e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8f1273328ec49cb9fcbbe4d1148d499": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b31a19cf294541c7823bd65584dc0aa8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b37e81525f1a4e5d9c72df74623abc33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c90b6d555b414422a581c46868e21f3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d72ee8a8142043ebbeee65093359f5f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93ee07cbc2f9448fa4f6e76b76f78ae8",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7749dfcf282c4a4892d11e87cda41aac",
      "value": 1
     }
    },
    "d7d7c472c38a4b48be349c63ce78f770": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}