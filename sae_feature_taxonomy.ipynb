{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ART-Organized SAE Feature Taxonomy\n",
    "\n",
    "Cluster pre-trained SAE decoder weight vectors from **Pythia-70M** using Adaptive Resonance Theory (ART) modules to build hierarchical feature taxonomies.\n",
    "\n",
    "**Pipeline:** SAE W_dec (32K × 512) → PCA → ART clustering → hierarchical taxonomy via SMART\n",
    "\n",
    "**Key idea:** ART's vigilance parameter provides a single, principled knob for controlling cluster granularity — something SAEs lack. SMART builds a hierarchy by stacking multiple vigilance levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Setup ---\nimport sys, os\n\nIN_COLAB = \"google.colab\" in sys.modules\nif IN_COLAB:\n    # Clone or pull the repo, then cd into it\n    if os.path.exists(\"art_utils.py\"):\n        # Already inside the repo (cell re-run)\n        !git pull\n    elif os.path.exists(\"ART-SAE-taxonomy\"):\n        os.chdir(\"ART-SAE-taxonomy\")\n        !git pull\n    else:\n        !git clone https://github.com/syre-ai/ART-SAE-taxonomy.git\n        os.chdir(\"ART-SAE-taxonomy\")\n    !pip install -q artlib==0.1.7 eai-sparsify transformers torch umap-learn plotly tqdm pandas\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom tqdm.auto import tqdm\n\nfrom art_utils import (\n    create_art_module,\n    create_smart_model,\n    extract_hierarchy_labels,\n    list_available_modules,\n)\n\nprint(\"Setup complete.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# CONFIGURATION — edit this cell to change experiments\n# ============================================================\n\nLAYER_IDX = 3              # Pythia-70M layer (0-5)\nMODULE_NAME = \"GPUFuzzyART\"  # GPU-accelerated FuzzyART (see list_available_modules())\nPARAM_OVERRIDES = {}       # e.g. {\"rho\": 0.8}\nPCA_DIMS = 50              # Reduce 512 dims to this; set None to skip PCA\nQUICK_TEST = True          # Subsample to 1000 features for fast iteration\nRANDOM_SEED = 42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load SAE decoder weights ---\n",
    "from sparsify import Sae\n",
    "\n",
    "saes = Sae.load_many(\"EleutherAI/sae-pythia-70m-32k\")\n",
    "layer_key = f\"layers.{LAYER_IDX}\"\n",
    "W_dec = saes[layer_key].W_dec.detach().numpy()  # (32768, 512)\n",
    "print(f\"Loaded W_dec for {layer_key}: shape {W_dec.shape}\")\n",
    "\n",
    "# Optional subsample for quick testing\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "if QUICK_TEST:\n",
    "    idx = rng.choice(W_dec.shape[0], size=1000, replace=False)\n",
    "    W_dec = W_dec[idx]\n",
    "    print(f\"Quick test mode: subsampled to {W_dec.shape[0]} features\")\n",
    "else:\n",
    "    # Shuffle to reduce ordering effects in ART\n",
    "    idx = rng.permutation(W_dec.shape[0])\n",
    "    W_dec = W_dec[idx]\n",
    "    print(f\"Shuffled {W_dec.shape[0]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Preprocessing: StandardScaler + optional PCA ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(W_dec)\n",
    "\n",
    "if PCA_DIMS is not None and PCA_DIMS < X_scaled.shape[1]:\n",
    "    pca = PCA(n_components=PCA_DIMS, random_state=RANDOM_SEED)\n",
    "    X_reduced = pca.fit_transform(X_scaled)\n",
    "    explained = pca.explained_variance_ratio_.sum()\n",
    "    print(f\"PCA: {X_scaled.shape[1]} → {PCA_DIMS} dims ({explained:.1%} variance explained)\")\n",
    "else:\n",
    "    X_reduced = X_scaled\n",
    "    print(f\"No PCA applied. Using {X_reduced.shape[1]} dims.\")\n",
    "\n",
    "data_dim = X_reduced.shape[1]\n",
    "print(f\"Final data shape: {X_reduced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create ART module ---\n",
    "list_available_modules()\n",
    "print()\n",
    "\n",
    "model = create_art_module(MODULE_NAME, dim=data_dim, overrides=PARAM_OVERRIDES)\n",
    "print(f\"\\nCreated {MODULE_NAME} for {data_dim}-dim data\")\n",
    "print(f\"Parameters: {model.params}\" if hasattr(model, 'params') else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Flat clustering ---\nX_prepared = model.prepare_data(X_reduced)\nprint(f\"Data after prepare_data: shape {X_prepared.shape}\")\n\nmodel.fit(X_prepared, verbose=True)\n\nlabels = model.labels_\nn_clusters = model.n_clusters\nprint(f\"\\nClusters found: {n_clusters}\")\nprint(f\"Samples: {len(labels)}\")\n\n# Cluster size stats\nunique, counts = np.unique(labels, return_counts=True)\nprint(f\"Cluster sizes — min: {counts.min()}, max: {counts.max()}, \"\n      f\"median: {np.median(counts):.0f}, mean: {counts.mean():.1f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- UMAP visualization of flat clusters ---\n",
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(n_components=2, random_state=RANDOM_SEED, n_neighbors=15)\n",
    "embedding = reducer.fit_transform(X_reduced)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "scatter = ax.scatter(\n",
    "    embedding[:, 0], embedding[:, 1],\n",
    "    c=labels, cmap=\"tab20\", s=5, alpha=0.6\n",
    ")\n",
    "ax.set_title(f\"{MODULE_NAME} Clusters (n={n_clusters}) — UMAP projection\")\n",
    "ax.set_xlabel(\"UMAP 1\")\n",
    "ax.set_ylabel(\"UMAP 2\")\n",
    "plt.colorbar(scatter, ax=ax, label=\"Cluster ID\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cluster size distribution ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].bar(range(len(counts)), sorted(counts, reverse=True), color=\"steelblue\")\n",
    "axes[0].set_xlabel(\"Cluster rank\")\n",
    "axes[0].set_ylabel(\"Size\")\n",
    "axes[0].set_title(\"Cluster sizes (sorted)\")\n",
    "\n",
    "axes[1].hist(counts, bins=30, color=\"steelblue\", edgecolor=\"white\")\n",
    "axes[1].set_xlabel(\"Cluster size\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(\"Cluster size histogram\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Hierarchical taxonomy with SMART ---\nrho_values = [0.3, 0.6, 0.85]\nsmart = create_smart_model(\n    MODULE_NAME, dim=data_dim, rho_values=rho_values, overrides=PARAM_OVERRIDES\n)\n\nX_smart = smart.prepare_data(X_reduced)\nsmart.fit(X_smart, verbose=True)\n\nhierarchy = extract_hierarchy_labels(smart)\nprint(f\"Hierarchy shape: {hierarchy.shape}  (samples × levels)\")\n\nfor i, rho in enumerate(rho_values):\n    n = len(np.unique(hierarchy[:, i]))\n    print(f\"  Level {i} (rho={rho}): {n} clusters\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sunburst visualization of hierarchy ---\n",
    "level_names = [f\"L{i}_rho{r}\" for i, r in enumerate(rho_values)]\n",
    "\n",
    "df_hier = pd.DataFrame(hierarchy, columns=level_names)\n",
    "# Convert cluster IDs to strings for plotly\n",
    "for col in level_names:\n",
    "    df_hier[col] = df_hier[col].astype(str)\n",
    "df_hier[\"count\"] = 1\n",
    "\n",
    "fig = px.sunburst(\n",
    "    df_hier,\n",
    "    path=level_names,\n",
    "    values=\"count\",\n",
    "    title=f\"SMART Hierarchy — {MODULE_NAME} base, rho={rho_values}\",\n",
    ")\n",
    "fig.update_layout(width=700, height=700)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature interpretation: top-activating tokens per cluster ---\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "lm = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "lm.eval()\n",
    "\n",
    "# Get the embedding matrix (vocab_size × d_model)\n",
    "embed_weights = lm.gpt_neox.embed_in.weight.detach()  # (50304, 512)\n",
    "\n",
    "# For each cluster, find tokens whose embeddings most align with the cluster's features\n",
    "# Use the original (non-PCA) decoder directions for interpretability\n",
    "sae = saes[f\"layers.{LAYER_IDX}\"]\n",
    "W_dec_full = sae.W_dec.detach()  # (32768, 512) on CPU\n",
    "\n",
    "n_show = min(5, n_clusters)  # Show top tokens for first few clusters\n",
    "top_k_tokens = 10\n",
    "\n",
    "print(f\"Top-{top_k_tokens} tokens for first {n_show} clusters:\\n\")\n",
    "for cluster_id in range(n_show):\n",
    "    mask = labels == cluster_id\n",
    "    if mask.sum() == 0:\n",
    "        continue\n",
    "\n",
    "    # Original feature indices (before subsampling/shuffling)\n",
    "    feature_indices = idx[mask] if QUICK_TEST else np.where(mask)[0]\n",
    "    cluster_dirs = W_dec_full[feature_indices]  # (n_features_in_cluster, 512)\n",
    "\n",
    "    # Mean direction of cluster\n",
    "    mean_dir = cluster_dirs.mean(dim=0)\n",
    "    mean_dir = mean_dir / mean_dir.norm()\n",
    "\n",
    "    # Cosine similarity with all token embeddings\n",
    "    embed_normed = embed_weights / embed_weights.norm(dim=1, keepdim=True).clamp(min=1e-8)\n",
    "    sims = embed_normed @ mean_dir\n",
    "    topk = sims.topk(top_k_tokens)\n",
    "\n",
    "    tokens = [tokenizer.decode([tid]) for tid in topk.indices.tolist()]\n",
    "    print(f\"Cluster {cluster_id} ({mask.sum()} features): {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vigilance parameter sweep (optional/advanced) ---\n",
    "# Sweep rho to see how cluster count changes with vigilance.\n",
    "# Higher rho → more clusters (tighter match required, except BayesianART).\n",
    "\n",
    "rho_sweep = np.linspace(0.1, 0.95, 10)\n",
    "cluster_counts = []\n",
    "\n",
    "for rho in tqdm(rho_sweep, desc=\"Vigilance sweep\"):\n",
    "    overrides_sweep = dict(PARAM_OVERRIDES, rho=rho)\n",
    "    m = create_art_module(MODULE_NAME, dim=data_dim, overrides=overrides_sweep)\n",
    "    X_p = m.prepare_data(X_reduced)\n",
    "    m.fit(X_p)\n",
    "    cluster_counts.append(m.n_clusters)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(rho_sweep, cluster_counts, \"o-\", color=\"steelblue\")\n",
    "ax.set_xlabel(\"Vigilance (rho)\")\n",
    "ax.set_ylabel(\"Number of clusters\")\n",
    "ax.set_title(f\"Vigilance sweep — {MODULE_NAME}\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for rho, nc in zip(rho_sweep, cluster_counts):\n",
    "    print(f\"  rho={rho:.2f} → {nc} clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Checkpoint Comparison (Stub)\n",
    "\n",
    "**Goal:** Track how SAE feature categories evolve across Pythia-70M training checkpoints.\n",
    "\n",
    "**Approach:**\n",
    "1. Load Pythia-70M at multiple checkpoints (e.g., steps 1000, 10000, 50000, 143000)\n",
    "2. Extract SAE decoder weights at each checkpoint\n",
    "3. Use ART's `partial_fit()` to incrementally update the taxonomy\n",
    "4. Track: resonant features (stable), new categories (plastic), dormant categories (lost)\n",
    "\n",
    "This leverages ART's core strength — stability-plasticity balance — which SAEs lack entirely (they must retrain from scratch).\n",
    "\n",
    "*Implementation in Phase 2.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}