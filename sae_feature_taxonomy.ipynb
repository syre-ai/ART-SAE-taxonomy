{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUIs_w6UZSoc"
   },
   "source": [
    "# ART-Organized SAE Feature Taxonomy\n",
    "\n",
    "Cluster pre-trained SAE decoder weight vectors from **Pythia** models using Adaptive Resonance Theory (ART) modules to build hierarchical feature taxonomies.\n",
    "\n",
    "**Pipeline:** SAE W_dec → optional PCA → ART clustering → hierarchical taxonomy via SMART\n",
    "\n",
    "**Key idea:** ART's vigilance parameter provides a single, principled knob for controlling cluster granularity — something SAEs lack. SMART builds a hierarchy by stacking multiple vigilance levels.\n",
    "\n",
    "**Supported models:** Pythia-70M (512-dim, 32k features), Pythia-410M (1024-dim, 65k features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvpA7ONAZSod",
    "outputId": "91fa49ee-f56d-4863-bd01-eccb51a1ebc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Setup ---\n",
    "import sys, os\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    # Clone or pull the repo, then cd into it\n",
    "    if os.path.exists(\"art_utils.py\"):\n",
    "        # Already inside the repo (cell re-run)\n",
    "        !git pull\n",
    "    elif os.path.exists(\"ART-SAE-taxonomy\"):\n",
    "        os.chdir(\"ART-SAE-taxonomy\")\n",
    "        !git pull\n",
    "    else:\n",
    "        !git clone https://github.com/syre-ai/ART-SAE-taxonomy.git\n",
    "        os.chdir(\"ART-SAE-taxonomy\")\n",
    "    !pip install -q artlib==0.1.7 eai-sparsify transformers torch umap-learn plotly tqdm pandas\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Force-reload local modules so git pull changes take effect\n",
    "import importlib\n",
    "import gpu_fuzzy_art, art_utils\n",
    "importlib.reload(gpu_fuzzy_art)\n",
    "importlib.reload(art_utils)\n",
    "\n",
    "from art_utils import (\n",
    "    create_art_module,\n",
    "    create_smart_model,\n",
    "    extract_hierarchy_labels,\n",
    "    list_available_modules,\n",
    ")\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6-WJVFqVZSoe"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION — edit this cell to change experiments\n",
    "# ============================================================\n",
    "\n",
    "# --- Model ---\n",
    "LM_MODEL = \"EleutherAI/pythia-410m\"     # LM for logit lens interpretation\n",
    "SAE_MODEL = \"EleutherAI/sae-pythia-410m-65k\"  # SAE decoder weights to cluster\n",
    "LAYER_IDX = 12             # Middle layer (410M has 24 layers, 70M has 6)\n",
    "\n",
    "# --- Clustering ---\n",
    "MODULE_NAME = \"GPUFuzzyART\"  # GPU-accelerated FuzzyART (see list_available_modules())\n",
    "PARAM_OVERRIDES = {\"rho\": 0.15}       # e.g. {\"rho\": 0.8}\n",
    "PCA_DIMS = None            # Set to int (e.g. 50, 100, 200) to reduce dims; None = use raw dims\n",
    "QUICK_TEST = True          # Subsample to 1000 features for fast iteration\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint restore disabled (USE_CHECKPOINT=False) — run full pipeline\n"
     ]
    }
   ],
   "source": [
    "# --- Checkpoint Restore (skip Cells 4-6 if checkpoint exists) ---\n",
    "# Set USE_CHECKPOINT = True to load saved pipeline state instead of recomputing.\n",
    "# This saves ~15 minutes by skipping SAE loading, logit lens, and junk filtering.\n",
    "\n",
    "import os, torch\n",
    "\n",
    "USE_CHECKPOINT = False  # Set True to restore from checkpoint\n",
    "CHECKPOINT_PATH = \"checkpoints/pipeline_state.pt\"\n",
    "\n",
    "if USE_CHECKPOINT and os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"Loading checkpoint from {CHECKPOINT_PATH}...\")\n",
    "    ckpt = torch.load(CHECKPOINT_PATH, weights_only=False)\n",
    "\n",
    "    # Verify config matches\n",
    "    ckpt_cfg = ckpt[\"config\"]\n",
    "    assert ckpt_cfg[\"lm_model\"] == LM_MODEL, \\\n",
    "        f\"Checkpoint LM mismatch: {ckpt_cfg['lm_model']} vs {LM_MODEL}\"\n",
    "    assert ckpt_cfg[\"sae_model\"] == SAE_MODEL, \\\n",
    "        f\"Checkpoint SAE mismatch: {ckpt_cfg['sae_model']} vs {SAE_MODEL}\"\n",
    "\n",
    "    # Restore tensors\n",
    "    W_dec_full = ckpt[\"W_dec_full\"]\n",
    "    top5_ids = ckpt[\"top5_ids\"]\n",
    "    unembed_weights = ckpt[\"unembed_weights\"]\n",
    "    final_ln_weight = ckpt[\"final_ln_weight\"]\n",
    "    final_ln_bias = ckpt[\"final_ln_bias\"]\n",
    "    valid_indices = ckpt[\"valid_indices\"]\n",
    "\n",
    "    # Rebuild final_ln as a simple module for logit lens\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LM_MODEL)\n",
    "    # Create a minimal layer norm with saved weights\n",
    "    import torch.nn as nn\n",
    "    final_ln = nn.LayerNorm(final_ln_weight.shape[0])\n",
    "    final_ln.weight.data = final_ln_weight\n",
    "    final_ln.bias.data = final_ln_bias\n",
    "    final_ln.eval()\n",
    "\n",
    "    # Rebuild filtered/subsampled data\n",
    "    import numpy as np\n",
    "    W_dec_raw = W_dec_full.numpy()\n",
    "    kept_indices = valid_indices.numpy()\n",
    "    W_dec_filtered = W_dec_raw[kept_indices]\n",
    "    rng = np.random.default_rng(RANDOM_SEED)\n",
    "    if QUICK_TEST:\n",
    "        n_sample = min(1000, W_dec_filtered.shape[0])\n",
    "        sub_idx = rng.choice(W_dec_filtered.shape[0], size=n_sample, replace=False)\n",
    "        idx_original = kept_indices[sub_idx]\n",
    "        W_dec = W_dec_filtered[sub_idx]\n",
    "    else:\n",
    "        perm = rng.permutation(W_dec_filtered.shape[0])\n",
    "        idx_original = kept_indices[perm]\n",
    "        W_dec = W_dec_filtered[perm]\n",
    "    idx = idx_original\n",
    "\n",
    "    # Run preprocessing (PCA) inline\n",
    "    if PCA_DIMS is not None and PCA_DIMS < W_dec.shape[1]:\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=PCA_DIMS, random_state=RANDOM_SEED)\n",
    "        X_reduced = pca.fit_transform(W_dec)\n",
    "    else:\n",
    "        X_reduced = W_dec.copy()\n",
    "    data_dim = X_reduced.shape[1]\n",
    "\n",
    "    print(f\"Checkpoint restored: {W_dec_full.shape[0]} features, \"\n",
    "          f\"{len(kept_indices)} valid, {X_reduced.shape[0]} selected, \"\n",
    "          f\"{data_dim}-dim\")\n",
    "    print(f\"Config: {ckpt_cfg}\")\n",
    "    print(\">>> Skip Cells 4-7 and proceed to Cell 8 (Create ART module)\")\n",
    "else:\n",
    "    if USE_CHECKPOINT:\n",
    "        print(f\"Checkpoint not found at {CHECKPOINT_PATH} — run full pipeline (Cells 4-7)\")\n",
    "    else:\n",
    "        print(\"Checkpoint restore disabled (USE_CHECKPOINT=False) — run full pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567,
     "referenced_widgets": [
      "7a769163cb994ee696540defcc9d8004",
      "5a242e5ceeb740c38e63698011acf3ef",
      "d72ee8a8142043ebbeee65093359f5f2",
      "761ec4428f4b4988b8fd52bc70ffb031",
      "d7d7c472c38a4b48be349c63ce78f770",
      "c90b6d555b414422a581c46868e21f3b",
      "b37e81525f1a4e5d9c72df74623abc33",
      "93ee07cbc2f9448fa4f6e76b76f78ae8",
      "7749dfcf282c4a4892d11e87cda41aac",
      "a8f1273328ec49cb9fcbbe4d1148d499",
      "9fb1b49baed04eccb905a69f14afc6e4",
      "5d445b318e01416d8581bbd483e7c834",
      "779842a0a3e84318882b30f4a46822fc",
      "1c61fba917ca465a8c80644c091fce75",
      "8b7c0b01735447538ac73a6c791abeac",
      "37456a9f41144975bb00ad364ebb58f2",
      "8c08137aac294db58eea77f93bc09ac2",
      "2f1111c1450745e9a44fe732351927c2",
      "3bcccf6d8db8493c8f5d500e54650a94",
      "517bbe701d874447a6f2156a65625cb3",
      "b31a19cf294541c7823bd65584dc0aa8",
      "4a8a6f8155b44f81b525c136c7735407"
     ]
    },
    "id": "hkGUSk8QZSof",
    "outputId": "51dad020-bd12-4e6f-d265-fbbae5f0f2aa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ee2bf48c2346ff91e2b3ee072b6a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 49 files:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dropping extra args {'signed': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded W_dec from EleutherAI/sae-pythia-410m-65k\n",
      "  Layer: layers.12.mlp, shape: (65536, 1024) (features × hidden_dim)\n",
      "  Available keys: ['layers.0.mlp', 'layers.1.mlp', 'layers.10.mlp', 'layers.11.mlp', 'layers.12.mlp', 'layers.13.mlp', 'layers.14.mlp', 'layers.15.mlp', 'layers.16.mlp', 'layers.17.mlp', 'layers.18.mlp', 'layers.19.mlp', 'layers.2.mlp', 'layers.20.mlp', 'layers.21.mlp', 'layers.22.mlp', 'layers.23.mlp', 'layers.3.mlp', 'layers.4.mlp', 'layers.5.mlp', 'layers.6.mlp', 'layers.7.mlp', 'layers.8.mlp', 'layers.9.mlp']\n"
     ]
    }
   ],
   "source": [
    "# --- Load SAE decoder weights ---\n",
    "from sparsify import Sae\n",
    "\n",
    "saes = Sae.load_many(SAE_MODEL)\n",
    "\n",
    "# Detect key format: \"layers.N\" (70M) vs \"layers.N.mlp\" (410M)\n",
    "layer_key = f\"layers.{LAYER_IDX}\"\n",
    "if layer_key not in saes:\n",
    "    layer_key = f\"layers.{LAYER_IDX}.mlp\"\n",
    "W_dec_raw = saes[layer_key].W_dec.detach().numpy()\n",
    "print(f\"Loaded W_dec from {SAE_MODEL}\")\n",
    "print(f\"  Layer: {layer_key}, shape: {W_dec_raw.shape} (features × hidden_dim)\")\n",
    "print(f\"  Available keys: {sorted(saes.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aced3a7017a147a494c24c369dd2b4ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26f73029d904aa3b90f2388f6306ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa20e632e8f41fb844a041f6977dbc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1884aae7e35d487689b1dd4d3954490f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fc63050a07401b9edfa91b252afb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/911M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM: EleutherAI/pythia-410m, vocab=50304, hidden=1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit lens: computed top-5 tokens for 65536 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Junk filter: 65536 total → 65534 kept, 2 filtered (0.0% junk)\n",
      "  0/5 junk tokens: 63560 features\n",
      "  1/5 junk tokens: 1935 features\n",
      "  2/5 junk tokens: 39 features\n",
      "  3/5 junk tokens: 2 features\n",
      "  4/5 junk tokens: 0 features\n",
      "  5/5 junk tokens: 0 features\n",
      "\n",
      "Quick test mode: subsampled to 1000 features (from 65534 non-junk)\n"
     ]
    }
   ],
   "source": [
    "# --- Filter junk features + subsample ---\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load LM (reused by Cell 11 for interpretation)\n",
    "tokenizer = AutoTokenizer.from_pretrained(LM_MODEL)\n",
    "lm = AutoModelForCausalLM.from_pretrained(LM_MODEL)\n",
    "lm.eval()\n",
    "lm.float()\n",
    "\n",
    "# Pythia models have UNTIED embeddings — use embed_out + final_layer_norm\n",
    "unembed_weights = lm.embed_out.weight.detach()  # (vocab_size, hidden_dim)\n",
    "final_ln = lm.gpt_neox.final_layer_norm\n",
    "print(f\"LM: {LM_MODEL}, vocab={unembed_weights.shape[0]}, hidden={unembed_weights.shape[1]}\")\n",
    "\n",
    "# --- Batch logit lens to classify all features (chunked to avoid OOM) ---\n",
    "W_dec_full = torch.from_numpy(W_dec_raw).float()\n",
    "n_features = W_dec_full.shape[0]\n",
    "top5_ids = torch.zeros(n_features, 5, dtype=torch.long)\n",
    "\n",
    "LOGIT_BATCH = 4096\n",
    "with torch.no_grad():\n",
    "    for start in range(0, n_features, LOGIT_BATCH):\n",
    "        end = min(start + LOGIT_BATCH, n_features)\n",
    "        normed = final_ln(W_dec_full[start:end])\n",
    "        logits = normed @ unembed_weights.T\n",
    "        top5_ids[start:end] = logits.topk(5, dim=1).indices\n",
    "        del normed, logits  # free memory between chunks\n",
    "print(f\"Logit lens: computed top-5 tokens for {n_features} features\")\n",
    "\n",
    "# --- Junk token classifier (minimal: only truly uninformative tokens) ---\n",
    "def is_junk_token(token_id):\n",
    "    decoded = tokenizer.decode([token_id])\n",
    "    if '\\ufffd' in decoded:        return True   # byte-fallback (broken encoding)\n",
    "    if '<|' in decoded:            return True   # special tokens (<|endoftext|> etc.)\n",
    "    return False\n",
    "\n",
    "# Classify each feature: junk if >=3 of top-5 tokens are junk\n",
    "junk_counts = torch.zeros(n_features, dtype=torch.long)\n",
    "for i in range(n_features):\n",
    "    junk_counts[i] = sum(is_junk_token(tid.item()) for tid in top5_ids[i])\n",
    "\n",
    "is_junk_feature = junk_counts >= 3  # >=60% junk\n",
    "keep_mask = ~is_junk_feature\n",
    "\n",
    "n_total = W_dec_raw.shape[0]\n",
    "n_kept = keep_mask.sum().item()\n",
    "n_filtered = n_total - n_kept\n",
    "print(f\"Junk filter: {n_total} total → {n_kept} kept, {n_filtered} filtered ({n_filtered/n_total:.1%} junk)\")\n",
    "\n",
    "# Distribution of junk counts\n",
    "for j in range(6):\n",
    "    nj = (junk_counts == j).sum().item()\n",
    "    print(f\"  {j}/5 junk tokens: {nj} features\")\n",
    "\n",
    "# --- Apply filter, then subsample/shuffle ---\n",
    "kept_indices = torch.where(keep_mask)[0].numpy()\n",
    "W_dec_filtered = W_dec_raw[kept_indices]\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "if QUICK_TEST:\n",
    "    n_sample = min(1000, W_dec_filtered.shape[0])\n",
    "    idx = rng.choice(W_dec_filtered.shape[0], size=n_sample, replace=False)\n",
    "    # Map back to original feature indices\n",
    "    idx_original = kept_indices[idx]\n",
    "    W_dec = W_dec_filtered[idx]\n",
    "    print(f\"\\nQuick test mode: subsampled to {W_dec.shape[0]} features (from {W_dec_filtered.shape[0]} non-junk)\")\n",
    "else:\n",
    "    perm = rng.permutation(W_dec_filtered.shape[0])\n",
    "    idx_original = kept_indices[perm]\n",
    "    W_dec = W_dec_filtered[perm]\n",
    "    print(f\"\\nShuffled {W_dec.shape[0]} non-junk features\")\n",
    "\n",
    "# idx tracks the original feature indices (for logit lens lookups in Cell 11)\n",
    "idx = idx_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LIyjTESZSof"
   },
   "outputs": [],
   "source": "# --- Preprocessing: optional PCA ---\n\nnorms = np.linalg.norm(W_dec, axis=1)\nprint(f\"W_dec L2 norms — min: {norms.min():.4f}, max: {norms.max():.4f}, \"\n      f\"mean: {norms.mean():.4f}, std: {norms.std():.6f}\")\n\nif PCA_DIMS is not None and PCA_DIMS < W_dec.shape[1]:\n    # Fit full PCA to report variance landscape, then truncate\n    max_components = min(W_dec.shape[0], W_dec.shape[1])\n    pca_full = PCA(n_components=min(max_components, max(PCA_DIMS, 200)), random_state=RANDOM_SEED)\n    pca_full.fit(W_dec)\n    cumvar = np.cumsum(pca_full.explained_variance_ratio_)\n\n    print(f\"Variance landscape ({len(cumvar)} components computed):\")\n    for thresh in [0.50, 0.80, 0.90, 0.95, 0.99]:\n        n_dims = np.searchsorted(cumvar, thresh) + 1\n        if n_dims <= len(cumvar):\n            print(f\"  {thresh:.0%} variance: {n_dims} dims\")\n        else:\n            print(f\"  {thresh:.0%} variance: >{len(cumvar)} dims (max computed: {cumvar[-1]:.1%})\")\n\n    # Use the requested number of dims\n    X_reduced = pca_full.transform(W_dec)[:, :PCA_DIMS]\n    explained = cumvar[PCA_DIMS - 1]\n    print(f\"\\nPCA: {W_dec.shape[1]} → {PCA_DIMS} dims ({explained:.1%} variance retained)\")\nelse:\n    X_reduced = W_dec.copy()\n    print(f\"No PCA applied. Using {X_reduced.shape[1]} dims.\")\n\ndata_dim = X_reduced.shape[1]\nprint(f\"Final data shape: {X_reduced.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to checkpoints/pipeline_state.pt (455.5 MB)\n",
      "  W_dec_full: torch.Size([65536, 1024])\n",
      "  top5_ids: torch.Size([65536, 5])\n",
      "  valid_indices: 65534 features\n",
      "To restore: set USE_CHECKPOINT=True in Cell 2, then run Cell 3\n"
     ]
    }
   ],
   "source": [
    "# --- Checkpoint Save ---\n",
    "# Saves pipeline state so future runs can skip Cells 4-6 (~15 min → ~3 sec).\n",
    "# Run this after Cells 4-6 complete successfully.\n",
    "\n",
    "import torch, os\n",
    "\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "CHECKPOINT_PATH = \"checkpoints/pipeline_state.pt\"\n",
    "\n",
    "checkpoint = {\n",
    "    \"W_dec_full\": W_dec_full,\n",
    "    \"top5_ids\": top5_ids,\n",
    "    \"unembed_weights\": unembed_weights,\n",
    "    \"final_ln_weight\": final_ln.weight.data.clone(),\n",
    "    \"final_ln_bias\": final_ln.bias.data.clone(),\n",
    "    \"valid_indices\": torch.from_numpy(kept_indices),\n",
    "    \"config\": {\n",
    "        \"lm_model\": LM_MODEL,\n",
    "        \"sae_model\": SAE_MODEL,\n",
    "        \"layer_idx\": LAYER_IDX,\n",
    "        \"n_features\": W_dec_full.shape[0],\n",
    "        \"hidden_dim\": W_dec_full.shape[1],\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, CHECKPOINT_PATH)\n",
    "file_size = os.path.getsize(CHECKPOINT_PATH) / (1024 * 1024)\n",
    "print(f\"Checkpoint saved to {CHECKPOINT_PATH} ({file_size:.1f} MB)\")\n",
    "print(f\"  W_dec_full: {W_dec_full.shape}\")\n",
    "print(f\"  top5_ids: {top5_ids.shape}\")\n",
    "print(f\"  valid_indices: {len(kept_indices)} features\")\n",
    "print(f\"To restore: set USE_CHECKPOINT=True in Cell 2, then run Cell 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "50u0PNLMZSof"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module             Complement Codes   Default Params\n",
      "--------------------------------------------------------------------------------\n",
      "FuzzyART           Yes                {'rho': 0.7, 'alpha': 0.01, 'beta': 1.0}\n",
      "GaussianART        No                 {'rho': 0.5, 'alpha': 1e-10} (+dim-dependent)\n",
      "HypersphereART     No                 {'rho': 0.7, 'alpha': 0.01, 'beta': 1.0, 'r_hat': 1.0}\n",
      "BayesianART        No                 {'rho': 0.01} (+dim-dependent)\n",
      "EllipsoidART       No                 {'rho': 0.7, 'alpha': 1e-07, 'beta': 1.0, 'mu': 0.8, 'r_hat': 1.0}\n",
      "GPUFuzzyART        No                 {'rho': 0.7, 'alpha': 0.01, 'beta': 1.0}\n",
      "\n",
      "\n",
      "Created GPUFuzzyART for 1024-dim data\n",
      "Parameters: {'rho': 0.15, 'alpha': 0.01, 'beta': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# --- Create ART module ---\n",
    "list_available_modules()\n",
    "print()\n",
    "\n",
    "model = create_art_module(MODULE_NAME, dim=data_dim, overrides=PARAM_OVERRIDES)\n",
    "print(f\"\\nCreated {MODULE_NAME} for {data_dim}-dim data\")\n",
    "print(f\"Parameters: {model.params}\" if hasattr(model, 'params') else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ksj67P6XZSof"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after prepare_data: shape (1000, 2048)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8bdfc540854c988791cbb3a83ea4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clustering:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clusters found: 6\n",
      "Samples: 1000\n",
      "Cluster sizes — min: 20, max: 202, median: 196, mean: 166.7\n"
     ]
    }
   ],
   "source": [
    "# --- Flat clustering ---\n",
    "X_prepared = model.prepare_data(X_reduced)\n",
    "print(f\"Data after prepare_data: shape {X_prepared.shape}\")\n",
    "\n",
    "model.fit(X_prepared, verbose=True)\n",
    "\n",
    "labels = model.labels_\n",
    "n_clusters = model.n_clusters\n",
    "print(f\"\\nClusters found: {n_clusters}\")\n",
    "print(f\"Samples: {len(labels)}\")\n",
    "\n",
    "# Cluster size stats\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(f\"Cluster sizes — min: {counts.min()}, max: {counts.max()}, \"\n",
    "      f\"median: {np.median(counts):.0f}, mean: {counts.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpMHsmeeZSog"
   },
   "outputs": [],
   "source": [
    "# --- UMAP visualization of flat clusters ---\n",
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(n_components=2, random_state=RANDOM_SEED, n_neighbors=15)\n",
    "embedding = reducer.fit_transform(X_reduced)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "scatter = ax.scatter(\n",
    "    embedding[:, 0], embedding[:, 1],\n",
    "    c=labels, cmap=\"tab20\", s=5, alpha=0.6\n",
    ")\n",
    "ax.set_title(f\"{MODULE_NAME} Clusters (n={n_clusters}) — UMAP projection\")\n",
    "ax.set_xlabel(\"UMAP 1\")\n",
    "ax.set_ylabel(\"UMAP 2\")\n",
    "plt.colorbar(scatter, ax=ax, label=\"Cluster ID\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZ4bYIZMZSog"
   },
   "outputs": [],
   "source": [
    "# --- Cluster size distribution ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].bar(range(len(counts)), sorted(counts, reverse=True), color=\"steelblue\")\n",
    "axes[0].set_xlabel(\"Cluster rank\")\n",
    "axes[0].set_ylabel(\"Size\")\n",
    "axes[0].set_title(\"Cluster sizes (sorted)\")\n",
    "\n",
    "axes[1].hist(counts, bins=30, color=\"steelblue\", edgecolor=\"white\")\n",
    "axes[1].set_xlabel(\"Cluster size\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(\"Cluster size histogram\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lZijdnpOZSog"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0da93175034efb9b0cdcc8a0853aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Level 0 (rho=0.1):   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f96a635d05141d5abd1260d990e49f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Level 1 (rho=0.4):   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22c36c1b0d7d458784958447235cd8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Level 2 (rho=0.7):   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "transient": {}
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchy shape: (1000, 3)  (samples × levels)\n",
      "  Level 0 (rho=0.1): 4 clusters\n",
      "  Level 1 (rho=0.4): 46 clusters\n",
      "  Level 2 (rho=0.7): 351 clusters\n"
     ]
    }
   ],
   "source": [
    "# --- Hierarchical taxonomy with SMART ---\n",
    "rho_values = [0.1, 0.4, 0.7]\n",
    "smart = create_smart_model(\n",
    "    MODULE_NAME, dim=data_dim, rho_values=rho_values, overrides=PARAM_OVERRIDES\n",
    ")\n",
    "\n",
    "X_smart = smart.prepare_data(X_reduced)\n",
    "smart.fit(X_smart, verbose=True)\n",
    "\n",
    "hierarchy = extract_hierarchy_labels(smart)\n",
    "print(f\"Hierarchy shape: {hierarchy.shape}  (samples × levels)\")\n",
    "\n",
    "for i, rho in enumerate(rho_values):\n",
    "    n = len(np.unique(hierarchy[:, i]))\n",
    "    print(f\"  Level {i} (rho={rho}): {n} clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Interpretation functions ---\nfrom collections import Counter\n\nglobal_mean_dir = W_dec_full.mean(dim=0)\n\ndef get_cluster_tokens(feature_indices, top_k=10):\n    \"\"\"Differential logit lens: top tokens distinctive to this cluster.\"\"\"\n    dirs = W_dec_full[feature_indices]\n    diff_dir = dirs.mean(dim=0) - global_mean_dir\n    with torch.no_grad():\n        normed = final_ln(diff_dir.unsqueeze(0))\n        logits = (normed @ unembed_weights.T).squeeze(0)\n    topk = logits.topk(top_k)\n    return [tokenizer.decode([tid]) for tid in topk.indices.tolist()]\n\ndef get_cluster_token_votes(feature_indices, top_k=10):\n    \"\"\"Count per-feature top-5 logit lens tokens across cluster.\"\"\"\n    counter = Counter()\n    for fi in feature_indices:\n        for tid in top5_ids[fi].tolist():\n            counter[tid] += 1\n    return [(tokenizer.decode([tid]).strip(), cnt) for tid, cnt in counter.most_common(top_k)]\n\nprint(\"Interpretation functions ready.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Results Summary (machine-parseable JSON) ---\nimport json as _json\n\n_l0_ids = sorted(np.unique(hierarchy[:, 0]))\n_cluster_summary = {}\nfor c in _l0_ids:\n    mask = hierarchy[:, 0] == c\n    feature_indices = idx[mask]\n    diff = get_cluster_tokens(feature_indices, top_k=5)\n    votes = get_cluster_token_votes(feature_indices, top_k=5)\n    _cluster_summary[int(c)] = {\n        \"size\": int(mask.sum()),\n        \"top_diff_tokens\": diff,\n        \"top_vote_tokens\": [tok for tok, cnt in votes],\n    }\n\nresults_summary = {\n    \"model\": SAE_MODEL,\n    \"layer\": LAYER_IDX,\n    \"module\": MODULE_NAME,\n    \"pca_dims\": PCA_DIMS,\n    \"quick_test\": QUICK_TEST,\n    \"n_features_clustered\": int(X_reduced.shape[0]),\n    \"rho_values\": rho_values,\n    \"n_clusters_per_level\": [\n        int(len(np.unique(hierarchy[:, i]))) for i in range(len(rho_values))\n    ],\n    \"clusters_L0\": _cluster_summary,\n}\n\nprint(\"===RESULTS_JSON===\")\nprint(_json.dumps(results_summary, indent=2))\nprint(\"===END_RESULTS===\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- L0 Taxonomy ---\nn_levels = hierarchy.shape[1]\nl0_ids = sorted(np.unique(hierarchy[:, 0]))\n\nprint(\"=\" * 70)\nprint(f\"LEVEL 0 — {len(l0_ids)} broad themes (rho={rho_values[0]})\")\nprint(\"=\" * 70)\n\nfor c in l0_ids:\n    mask = hierarchy[:, 0] == c\n    n_feat = mask.sum()\n    feature_indices = idx[mask]\n\n    child_info = \"\"\n    if n_levels > 1:\n        n_l1 = len(np.unique(hierarchy[mask, 1]))\n        child_info = f\", {n_l1} L1 children\"\n\n    diff_tokens = get_cluster_tokens(feature_indices, top_k=15)\n    votes = get_cluster_token_votes(feature_indices, top_k=10)\n\n    print(f\"\\nL0:{c} ({n_feat} features{child_info}):\")\n    print(f\"  Differential: {diff_tokens}\")\n    print(f\"  Votes:        {', '.join(f'{t}({cnt})' for t, cnt in votes)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueLAu-g7ZSog"
   },
   "outputs": [],
   "source": "# --- Explore hierarchy: drill into any cluster's children ---\n\ndef explore_cluster(level, cluster_id, top_k=10, max_children=50):\n    \"\"\"Show a cluster and list its children at the next level.\"\"\"\n    mask = hierarchy[:, level] == cluster_id\n    n = mask.sum()\n    if n == 0:\n        print(f\"No samples in L{level} cluster {cluster_id}\")\n        return\n\n    feature_indices = idx[mask]\n    diff_tokens = get_cluster_tokens(feature_indices, top_k=top_k)\n    votes = get_cluster_token_votes(feature_indices, top_k=top_k)\n\n    print(f\"L{level}:{cluster_id} ({n} features, rho={rho_values[level]}):\")\n    print(f\"  Differential: {diff_tokens}\")\n    print(f\"  Votes:        {', '.join(f'{t}({cnt})' for t, cnt in votes)}\")\n\n    child_level = level + 1\n    if child_level >= hierarchy.shape[1]:\n        print(\"  (finest level — no children)\")\n        return\n\n    child_ids, child_counts = np.unique(hierarchy[mask, child_level], return_counts=True)\n    order = np.argsort(-child_counts)\n    print(f\"\\n  -> {len(child_ids)} children at L{child_level} (rho={rho_values[child_level]}):\\n\")\n\n    for ci in order[:max_children]:\n        cid = child_ids[ci]\n        child_mask = mask & (hierarchy[:, child_level] == cid)\n        child_features = idx[child_mask]\n        child_diff = get_cluster_tokens(child_features, top_k=top_k)\n        child_votes = get_cluster_token_votes(child_features, top_k=5)\n        vote_str = ', '.join(f'{t}({cnt})' for t, cnt in child_votes[:5])\n        print(f\"    L{child_level}:{cid} ({child_counts[ci]} feat): {child_diff}\")\n        print(f\"      Votes: {vote_str}\")\n\n    if len(child_ids) > max_children:\n        print(f\"    ... and {len(child_ids) - max_children} more\")\n\n# Example: explore Level 0, Cluster 0\nexplore_cluster(level=0, cluster_id=0)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhqQHZC4ZSoh"
   },
   "outputs": [],
   "source": [
    "# --- Vigilance parameter sweep (optional/advanced) ---\n",
    "# Sweep rho to see how cluster count changes with vigilance.\n",
    "# Higher rho → more clusters (tighter match required, except BayesianART).\n",
    "\n",
    "rho_sweep = np.linspace(0.1, 0.95, 10)\n",
    "cluster_counts = []\n",
    "\n",
    "for rho in tqdm(rho_sweep, desc=\"Vigilance sweep\"):\n",
    "    overrides_sweep = dict(PARAM_OVERRIDES, rho=rho)\n",
    "    m = create_art_module(MODULE_NAME, dim=data_dim, overrides=overrides_sweep)\n",
    "    X_p = m.prepare_data(X_reduced)\n",
    "    m.fit(X_p)\n",
    "    cluster_counts.append(m.n_clusters)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(rho_sweep, cluster_counts, \"o-\", color=\"steelblue\")\n",
    "ax.set_xlabel(\"Vigilance (rho)\")\n",
    "ax.set_ylabel(\"Number of clusters\")\n",
    "ax.set_title(f\"Vigilance sweep — {MODULE_NAME}\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for rho, nc in zip(rho_sweep, cluster_counts):\n",
    "    print(f\"  rho={rho:.2f} → {nc} clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBlRV1R6ZSoh"
   },
   "source": [
    "## Phase 2: Checkpoint Comparison (Stub)\n",
    "\n",
    "**Goal:** Track how SAE feature categories evolve across Pythia-70M training checkpoints.\n",
    "\n",
    "**Approach:**\n",
    "1. Load Pythia-70M at multiple checkpoints (e.g., steps 1000, 10000, 50000, 143000)\n",
    "2. Extract SAE decoder weights at each checkpoint\n",
    "3. Use ART's `partial_fit()` to incrementally update the taxonomy\n",
    "4. Track: resonant features (stable), new categories (plastic), dormant categories (lost)\n",
    "\n",
    "This leverages ART's core strength — stability-plasticity balance — which SAEs lack entirely (they must retrain from scratch).\n",
    "\n",
    "*Implementation in Phase 2.*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (GPU)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1c61fba917ca465a8c80644c091fce75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3bcccf6d8db8493c8f5d500e54650a94",
      "max": 38,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_517bbe701d874447a6f2156a65625cb3",
      "value": 38
     }
    },
    "2f1111c1450745e9a44fe732351927c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "37456a9f41144975bb00ad364ebb58f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3bcccf6d8db8493c8f5d500e54650a94": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a8a6f8155b44f81b525c136c7735407": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "517bbe701d874447a6f2156a65625cb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5a242e5ceeb740c38e63698011acf3ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c90b6d555b414422a581c46868e21f3b",
      "placeholder": "​",
      "style": "IPY_MODEL_b37e81525f1a4e5d9c72df74623abc33",
      "value": "Download complete: "
     }
    },
    "5d445b318e01416d8581bbd483e7c834": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_779842a0a3e84318882b30f4a46822fc",
       "IPY_MODEL_1c61fba917ca465a8c80644c091fce75",
       "IPY_MODEL_8b7c0b01735447538ac73a6c791abeac"
      ],
      "layout": "IPY_MODEL_37456a9f41144975bb00ad364ebb58f2"
     }
    },
    "761ec4428f4b4988b8fd52bc70ffb031": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8f1273328ec49cb9fcbbe4d1148d499",
      "placeholder": "​",
      "style": "IPY_MODEL_9fb1b49baed04eccb905a69f14afc6e4",
      "value": " 2.42G/? [00:40&lt;00:00, 128MB/s]"
     }
    },
    "7749dfcf282c4a4892d11e87cda41aac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "779842a0a3e84318882b30f4a46822fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c08137aac294db58eea77f93bc09ac2",
      "placeholder": "​",
      "style": "IPY_MODEL_2f1111c1450745e9a44fe732351927c2",
      "value": "Fetching 38 files: 100%"
     }
    },
    "7a769163cb994ee696540defcc9d8004": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5a242e5ceeb740c38e63698011acf3ef",
       "IPY_MODEL_d72ee8a8142043ebbeee65093359f5f2",
       "IPY_MODEL_761ec4428f4b4988b8fd52bc70ffb031"
      ],
      "layout": "IPY_MODEL_d7d7c472c38a4b48be349c63ce78f770"
     }
    },
    "8b7c0b01735447538ac73a6c791abeac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b31a19cf294541c7823bd65584dc0aa8",
      "placeholder": "​",
      "style": "IPY_MODEL_4a8a6f8155b44f81b525c136c7735407",
      "value": " 38/38 [00:25&lt;00:00,  2.05it/s]"
     }
    },
    "8c08137aac294db58eea77f93bc09ac2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93ee07cbc2f9448fa4f6e76b76f78ae8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "9fb1b49baed04eccb905a69f14afc6e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8f1273328ec49cb9fcbbe4d1148d499": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b31a19cf294541c7823bd65584dc0aa8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b37e81525f1a4e5d9c72df74623abc33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c90b6d555b414422a581c46868e21f3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d72ee8a8142043ebbeee65093359f5f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93ee07cbc2f9448fa4f6e76b76f78ae8",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7749dfcf282c4a4892d11e87cda41aac",
      "value": 1
     }
    },
    "d7d7c472c38a4b48be349c63ce78f770": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}